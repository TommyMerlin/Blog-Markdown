---
title: 统计学习方法第七章—支持向量机
date: 2020-01-09 19:20:00
tags:
- 机器学习
- 统计学习方法
- 监督学习
categories:
- 机器学习
- 统计学习方法
mathjax: true
---

<img src="https://aliyun-oss-coderhuye.oss-cn-hangzhou.aliyuncs.com/blog/2021-06-19-%E6%94%AF%E6%8C%81%E5%90%91%E9%87%8F%E6%9C%BA-be5f6d.png" alt="支持向量机" width="50%" />

<!-- more -->

## 1.线性可分支持向量机与硬间隔最大化

### 1.1 线性可分支持向量机
**支持向量机：二类分类**

在特征空间中找到以一个分离超平面将实例分到不同的类。

> 感知机：误分类最小策略，解无穷多。  
> 线性可分支持向量机：间隔最大化策略，解唯一。

### 1.2 函数间隔和几何间隔
**函数间隔**：表示分类预测的正确性及确信度，超平面 $(w,b)$ 关于样本点 $(x_i,y_i)$ 的函数间隔为
$$
\hat{\gamma}_{i}=y_{i}\left(w \cdot x_{i}+b\right)
$$

超平面 $(w,b)$ 关于训练数据集 T 的函数间隔为超平面 $(w,b)$ 关于 T 中所有样本点 $(x_i,y_i)$ 的函数间隔之最小值，即
$$
\hat{\gamma}=\min _{i=1, \cdots, N} \hat{\gamma}_{i}
$$

**几何间隔**：对分离超平面的法向量w加上规范化约束，使得间隔为确定值，此时函数间隔→几何间隔，超平面 $(w,b)$ 关于样本点 $(x_i,y_i)$ 的几何间隔为
$$
\gamma_{i}=y_{i}\left(\frac{w}{\|w\|} \cdot x_{i}+\frac{b}{\|w\|}\right)
$$
其中 $\|w\|$ 为 w 的 $L_2$ 范数。

<img src="https://aliyun-oss-coderhuye.oss-cn-hangzhou.aliyuncs.com/blog/2021-06-19-%E5%87%A0%E4%BD%95%E9%97%B4%E9%9A%94-990981.png" alt="几何间隔" width="30%" />

超平面 $(w,b)$ 关于训练数据集 T 的几何间隔为超平面 $(w,b)$ 关于 T 中所有样本点 $(x_i,y_i)$ 的几何间隔之最小值，即
$$
\gamma=\min _{i=1, \cdots, N} \gamma_{i}
$$
函数间隔和几何间隔的**关系**：
$$
\begin{aligned}
\gamma_{i}&=\frac{\hat{\gamma}_{i}}{\|w\|}\\
\gamma&=\frac{\hat{\gamma}}{\|w\|}
\end{aligned}
$$

### 1.3 间隔最大化

支持向量机**基本思想**：求解能够正确划分训练集并且几何间隔最大的分离超平面。

#### 1.3.1最大间隔分离超平面

求解几何间隔最大的分离超平面即为**求解约束最优化问题**：
$$
\begin{array}{ll}{\max _{w, b}} & {\frac{\hat{\gamma}}{\|w\|}} \\ {\text { s.t. }} & {y_{i}\left(w \cdot x_{i}+b\right) \geqslant \hat{\gamma}, \quad i=1,2, \cdots, N}\end{array}
$$

函数间隔 $\hat{\gamma}$  的取值不影响最优化问题的解，故可取 $\hat{\gamma}=1$ ，由于最大化  $\frac{1}{\|w\|}$ 和最小化 $\frac{1}{2}{\|w\|}^2$ 是等价的，可将原问题转化为下面的最优化问题（**原始问题**）：
$$
\begin{array}{ll}{\min _{w, b}} & {\frac{1}{2}\|w\|^{2}} \\ {\text { s.t. }} & {y_{i}\left(w \cdot x_{i}+b\right)-1 \geqslant 0, \quad i=1,2, \cdots, N}\end{array}
$$
求得最优解 $w^*$,$b^*$，就可得到最大间隔分离超平面 $w^*\cdot x+b^*=0$ 及分类决策函数 $f(x)=sign(w^*\cdot x+b^*)$ 。

#### 1.3.2支持向量和间隔边界

**支持向量**：与分离超平面距离最近的样本点的实例，满足 $y_{i}\left(w \cdot x_{i}+b\right)-1=0$ 。

<img src="https://aliyun-oss-coderhuye.oss-cn-hangzhou.aliyuncs.com/blog/2021-06-19-%E6%94%AF%E6%8C%81%E5%90%91%E9%87%8F-77095e.png" alt="支持向量" width="30%" />

**间隔边界**：$H_1$ 和 $H_2$，其中
$$
\begin{aligned}
H_{1}&: w \cdot x+b=1\\
H_{2}&: w \cdot x+b=-1
\end{aligned}
$$
**间隔**：$H_1$ 和 $H_2$ 之间的距离，等于 $\frac{2}{\|w\|}$ 。

### 1.4学习的对偶算法

**对偶问题**：
$$
\begin{array}{cl}{\min _{\alpha}} & {\frac{1}{2} \sum_{i=1}^{N} \sum_{j=1}^{N} \alpha_{i} \alpha_{j} y_{i} y_{j}\left(x_{i} \cdot x_{j}\right)-\sum_{i=1}^{N} \alpha_{i}} \\ {\text { s.t. }} & {\sum_{i=1}^{N} \alpha_{i} y_{i}=0} \\ {} & {\alpha_{i} \geqslant 0, \quad i=1,2, \cdots, N}\end{array}
$$
设 $\alpha^{*}=\left(\alpha_{1}^{*}, \alpha_{2}^{*}, \cdots, \alpha_{l}^{*}\right)^{\mathrm{T}}$ 是对偶问题的解，则存在下标 j ，使得 $\alpha_{j}^{*}>0$ ，并可通过下式求得原始问题的解 $w^*$,$b^*$：
$$
\begin{aligned} w^{*} &=\sum_{i=1}^{N} \alpha_{i}^{*} y_{i} x_{i} \\ b^{*}&=y_{j} -\sum_{i=1}^{N} \alpha_{i}^{*} y_{i}\left(x_{i} \cdot x_{j}\right) \end{aligned}
$$
**支持向量**：训练数据集中对应于 $\alpha_{i}^{*}>0$ 的样本点 $(x_i,y_i)$ 的实例 $x_i\in R^n$ ，也即  $y_{i}\left(w \cdot x_{i}+b\right)-1=0$ 。

## 2.线性支持向量机与软间隔最大化

### 2.1线性支持向量机

训练集中存在一些**特异点**，即这些样本点不满足函数间隔大于等于1的约束条件，为解决这一问题，可对每个样本点 $(x_i,y_i)$ 引入一个**松弛变量**  $\xi_{i} \geqslant 0$ ，是函数间隔大于等于1，此时约束条件变为：
$$
y_{i}\left(w \cdot x_{i}+b\right) \geqslant 1-\xi_{i}
$$
目标函数变为：
$$
\frac{1}{2}\|w\|^{2}+C \sum_{i=1}^{N} \xi_{i}
$$
其中，$C>0$ 称为**惩罚函数**，C值越大，对误分类的惩罚越大。

**最小化目标函数的含义**：

- 使 $\frac{1}{2}\|w\|^{2}$ 尽量小，即间隔尽量大
- 使误分类点的个数尽量小

**原始问题**：
$$
\begin{array}{ll}{\min _{w, b, \xi}} & {\frac{1}{2}\|w\|^{2}+C \sum_{i=1}^{N} \xi_{i}} \\ {\text { s.t. }} & {y_{i}\left(w \cdot x_{i}+b\right) \geqslant 1-\xi_{i}, \quad i=1,2, \cdots, N} \\ {} & {\xi_{i} \geqslant 0, \quad i=1,2, \cdots, N}\end{array}
$$


w的解唯一，b的解存在于一个区间。

**对偶问题**：
$$
\begin{array}{ll}{\min _{\alpha}} & {\frac{1}{2} \sum_{i=1}^{N} \sum_{j=1}^{N} \alpha_{i} \alpha_{j} y_{i} y_{j}\left(x_{i} \cdot x_{j}\right)-\sum_{i=1}^{N} \alpha_{i}} \\ {\text { s.t. }} & {\sum_{i=1}^{N} \alpha_{i} y_{i}=0} \\ {} & {0 \leqslant \alpha_{i} \leqslant C, \quad i=1,2, \cdots, N}\end{array}
$$
设 $\alpha^{*}=\left(\alpha_{1}^{*}, \alpha_{2}^{*}, \cdots, \alpha_{l}^{*}\right)^{\mathrm{T}}$ 是对偶问题的解，若存在 $\alpha^*$ 的一个分量 $\alpha^*_j$ ，$0<\alpha^*_j<C$，则可通过下式求得原始问题的解 $w^*$,$b^*$：
$$
\begin{aligned}
w^{*}&=\sum_{i=1}^{N} \alpha_{i}^{*} y_{i} x_{i} \\ 
b^{*}&=y_{j}-\sum_{i=1}^{N} y_{i} \alpha_{i}^{*}(x_{i}\cdot x_{j})
\end{aligned}
$$
由于 b 的解不唯一，可在满足条件的样本点上取平均值。

### 2.2支持向量

**支持向量**：对偶问题的解 $\alpha^{*}=\left(\alpha_{1}^{*}, \alpha_{2}^{*}, \cdots, \alpha_{l}^{*}\right)^{\mathrm{T}}$ 中对应于 $\alpha^*_i>0$ 的样本点的实例。

<img src="https://aliyun-oss-coderhuye.oss-cn-hangzhou.aliyuncs.com/blog/2021-06-19-%E8%BD%AF%E9%97%B4%E9%9A%94%E7%9A%84%E6%94%AF%E6%8C%81%E5%90%91%E9%87%8F-82a789.png" alt="软间隔的支持向量" width="30%" />

**$\alpha^*$ 不同大小代表支持向量不同位置：**

-  若 $\alpha^*_i>C$ ，则 $\xi_i=0$ ，支持向量 $x_i$ 恰好落在间隔边界上。
-  若 $\alpha^*_i=C$ ，则 $0<\xi_i<0$ ，$x_i$ 在间隔边界与分离超平面之间。
-  若 $\alpha^*_i=C$ ，则 $\xi_i=1$ ，支持向量 $x_i$ 在分离超平面上。
-  若 $\alpha^*_i=C$ ，则 $\xi_i>0$ ，支持向量 $x_i$ 在分离超平面误分一侧。