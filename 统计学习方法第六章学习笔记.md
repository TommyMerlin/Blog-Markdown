---
title: 统计学习方法第六章—逻辑斯谛回归与最大熵模型
date: 2020-01-09 19:20:00
tags:
- 机器学习
- 统计学习方法
- 监督学习
categories:
- 机器学习
- 统计学习方法
mathjax: true
---

## 1.逻辑斯谛回归模型
### 1.1 逻辑斯谛分布
**逻辑斯谛分布**：X是连续随机变量，X服从逻辑斯谛分布指其具有下列分布函数和密度函数：
$$
\begin{array}{l}{F(x)=P(X \leqslant x)=\frac{1}{1+\mathrm{e}^{-(x-\mu) / \gamma}}} \\ {f(x)=F^{\prime}(x)=\frac{\mathrm{e}^{-(x-\mu) / \gamma}}{\gamma\left(1+\mathrm{e}^{-(x-\mu) / \gamma}\right)^{2}}}\end{array}
$$
其中 $\mu$ 为位置参数，$\gamma>0$ 为形状参数。

<!-- more -->

f(x) 与 F(x) 图形如下图所示，为 S 形曲线(sigmoid curve)，以点 $(\mu,\frac{1}{2})$ 中心对称。$\gamma$ 越小 ，曲线在中心附近增长得越快。

<img src="https://aliyun-oss-coderhuye.oss-cn-hangzhou.aliyuncs.com/blog/2021-06-19-%E9%80%BB%E8%BE%91%E6%96%AF%E8%B0%9B%E5%88%86%E5%B8%83%E5%AF%86%E5%BA%A6%E5%87%BD%E6%95%B0-358e85.png" alt="逻辑斯谛分布密度函数" width="50%" />

### 1.2 二项逻辑斯谛回归模型

**逻辑斯谛回归模型**：如下条件概率分布：
$$
\begin{array}{l}{P(Y=1 | x)=\frac{\exp (w \cdot x)}{1+\exp (w \cdot x)}} \\ {P(Y=0 | x)=\frac{1}{1+\exp (w \cdot x)}}\end{array}
$$
其中 $w=\left(w^{(1)}, w^{(2)}, \cdots, w^{(n)}, b\right)^{\mathrm{T}}$ ， $x=\left(x^{(1)}, x^{(2)}, \cdots, x^{(n)}, 1\right)^{\mathrm{T}}$。

逻辑斯谛回归比较两个条件概率值的大小，将实例 x 分到概率值较大的那一类。

一个事件(发生概率为p)的对数几率或 logit 函数是：
$$
\operatorname{logit}(p)=\log \frac{p}{1-p}
$$
对于逻辑斯谛回归，可得
$$
\log \frac{P(Y=1 | x)}{1-P(Y=1 | x)}=w \cdot x
$$
即输出 Y=1 的对数几率是输入 x 的线性函数。

### 1.3 模型参数估计

极大似然估计法

令 $P(Y=1 | x)=\pi(x),\ P(Y=0 | x)=1-\pi(x)$ 

似然函数为 $\prod_{i=1}^{N}\left[\pi\left(x_{i}\right)\right]^{y_{i}}\left[1-\pi\left(x_{i}\right)\right]^{1-y_{i}}$ 

对数似然函数为
$$
L(w) =\sum_{i=1}^{N}[y_{i}\left(w \cdot x_{i}\right)-\log \left(1+\exp \left(w \cdot x_{i}\right)\right]
$$
对 L(w) 求极大值，得到 w 的估计值：

- 梯度下降法
- 拟牛顿法

## 2.最大熵模型

### 2.1 最大熵原理

**最大熵原理**：熵最大的模型是最好的模型，当 X 服从均匀分布时，熵最大。

### 2.2 最大熵模型定义

